# ==========================================================
# ğŸ“˜ GuÃ­a prÃ¡ctica de Modelos de RegresiÃ³n y Funciones Ãštiles
# ==========================================================
# Esta guÃ­a resume los principales modelos de regresiÃ³n y funciones auxiliares
# que se pueden usar en anÃ¡lisis de datos y machine learning.
# Es un apunte de consulta rÃ¡pida.

from sklearn.linear_model import (
    HuberRegressor, RANSACRegressor, TheilSenRegressor,
    LinearRegression, RidgeCV
)
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import LinearSVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# ----------------------------------------------------------
# 1ï¸âƒ£ TIPOS DE REGRESORES
# ----------------------------------------------------------

# ğŸ“ˆ RegresiÃ³n lineal estÃ¡ndar
modelo = LinearRegression()
# âœ” Ajusta una recta/hiperplano que minimiza el error cuadrÃ¡tico.
# âŒ Muy sensible a outliers.
# âœ… Ãštil cuando la relaciÃ³n es lineal y los datos son â€œlimpiosâ€.

# ğŸ›¡ï¸ Regresores robustos
modelo = HuberRegressor()
# âœ” Combina MSE y MAE â†’ tolera outliers leves.

modelo = RANSACRegressor()
# âœ” Ignora outliers extremos y ajusta solo inliers.

modelo = TheilSenRegressor()
# âœ” Usa medianas de pendientes, muy robusto a outliers.
# ğŸ’¡ RecomendaciÃ³n:
#   - Muchos outliers   â†’ RANSAC
#   - Pocos pero fuertes â†’ Huber o TheilSen
#   - Sin outliers      â†’ LinearRegression

# ğŸŒ² Modelos adicionales
modelo = ExtraTreesRegressor()
# âœ” Ensamble de Ã¡rboles aleatorios, captura no linealidades.

modelo = KNeighborsRegressor()
# âœ” Predice con vecinos cercanos (promedio).

modelo = RidgeCV()
# âœ” RegresiÃ³n lineal con regularizaciÃ³n L2 (reduce sobreajuste).

# ğŸ¤– Support Vector Regression (SVR)
modelo = LinearSVR()
# âœ” Ajusta una recta tolerando un margen de error (epsilon).
# âœ” Robusta frente a ruido.
# ğŸ’¡ Para mÃºltiples salidas: usar MultiOutputRegressor(LinearSVR()).

# ----------------------------------------------------------
# 2ï¸âƒ£ FUNCIONES AUXILIARES
# ----------------------------------------------------------

# ğŸ“Š CorrelaciÃ³n de Pearson
def coef_corr(x, y):
    arriba = sum((x - x.mean()) * (y - y.mean()))
    abajo = sum((x - x.mean())**2) * sum((y - y.mean())**2)
    return arriba / np.sqrt(abajo)
# ğŸ“Œ Mide la relaciÃ³n lineal entre dos variables:
#   1  â†’ correlaciÃ³n perfecta positiva
#  -1  â†’ correlaciÃ³n perfecta negativa
#   0  â†’ no hay relaciÃ³n lineal

# ğŸ“ MÃ©tricas de error
# mse = mean_squared_error(y_true, y_pred)
# rmse = np.sqrt(mse)
# mae = mean_absolute_error(y_true, y_pred)
# ğŸ“Œ MAE = error absoluto promedio
# ğŸ“Œ RMSE = penaliza mÃ¡s los errores grandes

# ğŸ“ Criterio de Akaike (AIC)
def AIC(ecm, num_params):
    return -2 * np.log(ecm) + 2 * num_params
# ğŸ“Œ Compara modelos: menor AIC = mejor ajuste con menos complejidad.

# ğŸ“ˆ RegresiÃ³n polinÃ³mica
pf = PolynomialFeatures(degree=3)
# X_poly = pf.fit_transform(X)
# modelo = LinearRegression()
# modelo.fit(X_poly, y)
# ğŸ“Œ Convierte relaciones no lineales en un espacio lineal con features polinÃ³micas.

# ğŸŒ€ FunciÃ³n sigmoide (curva logÃ­stica)
def sigmoid(x, Beta_1, Beta_2):
    return 1 / (1 + np.exp(-Beta_1 * (x - Beta_2)))
# from scipy.optimize import curve_fit
# popt, pcov = curve_fit(sigmoid, xdata, ydata)
# ğŸ“Œ Ãštil cuando la curva de crecimiento sigue forma de "S" (ej. poblaciÃ³n).

# ----------------------------------------------------------
# 3ï¸âƒ£ VISUALIZACIÃ“N
# ----------------------------------------------------------

def plot_best_fit(X, y, xaxis, model):
    """Entrena un modelo y dibuja la recta de mejor ajuste."""
    model.fit(X, y)
    yaxis = model.predict(xaxis.reshape((len(xaxis), 1)))
    plt.plot(xaxis, yaxis, label=type(model).__name__)
    plt.scatter(X, y, color='blue', alpha=0.5)
    plt.legend()
    plt.show()
# ğŸ“Œ Sirve para comparar visualmente distintos modelos.

# ----------------------------------------------------------
# 4ï¸âƒ£ CUÃNDO USAR CADA MODELO
# ----------------------------------------------------------
# âœ… Datos lineales, sin outliers        â†’ LinearRegression o Ridge
# âœ… Datos lineales, pocos outliers      â†’ Huber o TheilSen
# âœ… Datos lineales, muchos outliers     â†’ RANSAC
# âœ… Relaciones no lineales simples      â†’ PolynomialFeatures + LinearRegression
# âœ… Relaciones complejas                â†’ ExtraTrees, KNN
# âœ… Datos con ruido en regresiÃ³n lineal â†’ LinearSVR
# âœ… Crecimiento en forma de S           â†’ sigmoid + curve_fit
